{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b401175",
   "metadata": {},
   "source": [
    "# ロボットアーム - Panda Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e009b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from gail_algo import GAILTrainer, build_expert_loader\n",
    "from collect_expert_trajectories import collect_expert_trajectories\n",
    "\n",
    "# 日本語フォント設定\n",
    "import matplotlib\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8bdd3",
   "metadata": {},
   "source": [
    "* ロボット: Franka Emika Panda マニピュレータの操作タスクをシミュレート。\n",
    "<br>\n",
    "\n",
    "* 観測空間:\n",
    "    - すべてのタスクにグリッパの位置と速度（6値）。\n",
    "    - 物体を扱うタスクでは位置・姿勢・線形/回転速度が追加（物体1つにつき12値）。\n",
    "    - グリッパ開閉距離（拘束されていなければ1値）。\n",
    "\n",
    "* 行動空間:\n",
    "    - グリッパの平行移動コマンド（x, y, z の3値）。\n",
    "    - グリッパ開閉コマンド（1値）。\n",
    "    \n",
    "* シミュレーション:\n",
    "    - エージェントの1ステップあたり20タイムステップ（各2ms）。\n",
    "    - インタラクション周波数は25Hz。\n",
    "    - ほとんどのタスクは約2秒（50ステップ）のエピソード長。\n",
    "    \n",
    "* 報酬関数:\n",
    "    - 既定の報酬はスパース: 目標到達（5cm 以内）なら0、それ以外は -1。\n",
    "    - スパース報酬は定義が簡単だが、進捗の手掛かりが少ない。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97ae06",
   "metadata": {},
   "source": [
    "# PandaReach-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82d3d3",
   "metadata": {},
   "source": [
    "* ターゲット位置は30cm × 30cm × 30cmの範囲でランダム生成され、グリッパがそこへ到達するタスク。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c34656",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"PandaReach-v3\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    renderer=\"OpenGL\",\n",
    "    render_target_position=[0, 0.15, 0.25],\n",
    "    render_distance=0.85,\n",
    "    render_yaw=135,\n",
    "    render_pitch=-20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n観測空間:', env.observation_space)\n",
    "print('\\n行動空間: ', env.action_space)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print('\\n初期状態: ', (obs, info))\n",
    "\n",
    "# 行動は環境のアクション空間と同じ形状にする（スカラーだとエラーになる）\n",
    "action_sample = np.zeros(env.action_space.shape, dtype=np.float32)\n",
    "print('\\n環境での1ステップ: ', env.step(action_sample))\n",
    "\n",
    "print('\\n\\nレンダリングした環境: ')\n",
    "env.reset()\n",
    "plt.axis('off')\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = env.observation_space['observation'].shape[0] + \\\n",
    "            env.observation_space['achieved_goal'].shape[0] + \\\n",
    "            env.observation_space['desired_goal'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aabd3",
   "metadata": {},
   "source": [
    "# GAIL用のエキスパート軌跡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd06b1",
   "metadata": {},
   "source": [
    "1. `../TD3/Models/Expert/` の事前学習TD3エキスパートで軌跡（状態 + 行動）をロールアウト。\n",
    "2. 生成した軌跡を一度 `./expert_trajectories.pt` に保存し、以降の実行で再利用。\n",
    "3. このファイルからミニバッチを作り、学習中にGAIL識別器へ供給する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867463e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_path = \"./expert_trajectories.pt\"\n",
    "if not os.path.exists(expert_path):\n",
    "    collect_expert_trajectories(env_name=\"PandaReach-v3\", episodes=200, steps_per_episode=300,\n",
    "                                expert_model_path=\"../TD3/Models/Expert/\", save_path=expert_path,\n",
    "                                render=False)\n",
    "else:\n",
    "    print(f\"キャッシュされたエキスパート軌跡を {expert_path} から利用します\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdeecab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expert_loader = build_expert_loader(expert_path, batch_size=256, device=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce94f4",
   "metadata": {},
   "source": [
    "# GAIL目的関数（数式）\n",
    "- **報酬（ポリシー側）**: $r(s,a) = \\log D_\\phi(s,a)$。識別器が「エキスパートらしい」と判断するほど報酬が高い。\n",
    "- **識別器の損失**（最小化で表記）: \n",
    "$$\\mathcal{L}_D = -\\mathbb{E}_{(s,a)\\sim\\text{expert}}[\\log D_\\phi(s,a)] - \\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log (1 - D_\\phi(s,a))]$$\n",
    "- **ポリシー（生成側）の目的**: \n",
    "$$\\max_\\theta\\; \\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log D_\\phi(s,a)]$$\n",
    "- **TD3本体**: Actor/Critic はTD3そのまま（2つのCriticで $\\min(Q_1, Q_2)$、ターゲット平滑化、遅延Actor更新）。GAILでは上記の $\\log D$ を外部報酬として TD3 学習に渡す点のみが異なる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAILエージェント設定（繰り返し実行用に辞書で保持）\n",
    "gail_agent_kwargs = dict(\n",
    "    env=env,\n",
    "    input_dims=obs_shape,\n",
    "    agent_name='GAIL',\n",
    "    model_save_path='./Models/Apprentice/',\n",
    "    exploration_period=300,\n",
    "    disc_lr=3e-4,\n",
    "    disc_updates=2,\n",
    "    gail_reward_scale=1.0,\n",
    "    expert_loader=expert_loader,\n",
    ")\n",
    "\n",
    "print('GAILエージェント設定を初期化しました。次のセルで繰り返し学習できます。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a59992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-train GAIL agents for a fixed number of runs (no interactive prompts)\n",
    "\n",
    "max_runs = 10  # number of sequential GAIL agents to train\n",
    "\n",
    "\n",
    "\n",
    "run_idx = 0\n",
    "\n",
    "\n",
    "\n",
    "results_dir = Path(\"../Results/GAIL\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "manifest_path = results_dir / \"run_manifest.json\"\n",
    "\n",
    "\n",
    "\n",
    "# Load existing manifest if it exists (resume-friendly)\n",
    "if manifest_path.exists():\n",
    "    try:\n",
    "        with open(manifest_path, \"r\") as f:\n",
    "            run_manifest = json.load(f)\n",
    "    except Exception:\n",
    "        run_manifest = []\n",
    "else:\n",
    "    run_manifest = []\n",
    "\n",
    "\n",
    "\n",
    "# Track per-run aggregates for a final summary\n",
    "per_run_mean_rewards = []  # mean episodic reward over evaluation rollouts\n",
    "per_run_success_rates = []  # success rate (%) over evaluation rollouts\n",
    "\n",
    "\n",
    "\n",
    "for run_idx in range(1, max_runs + 1):\n",
    "\n",
    "    # Separate checkpoint directories per run to avoid overwriting\n",
    "    gail_agent_kwargs['model_save_path'] = f\"./Models/Apprentice_Run_{run_idx}/\"\n",
    "\n",
    "    agent = GAILTrainer(**gail_agent_kwargs)\n",
    "\n",
    "    # Train the agent once; each run is independent\n",
    "    score_history, avg_score_history, success_history, avg_success_history = agent.gail_train(\n",
    "        n_episodes=500,\n",
    "        opt_steps=10,\n",
    "        print_every=50,\n",
    "        render_save_path=None,\n",
    "        plot_save_path=f\"../Results/GAIL/GAIL_Performance_Run_{run_idx}.png\",\n",
    "        plot_title=f\"GAIL Run {run_idx} Learning Curve\",\n",
    "    )\n",
    "\n",
    "    agent.save_model()\n",
    "    print(f\"Run {run_idx} を保存しました -> {gail_agent_kwargs['model_save_path']}\")\n",
    "\n",
    "    # Post-training evaluation across multiple episodes to compute success rate\n",
    "    eval_returns = []\n",
    "    eval_successes = []\n",
    "    for _ in range(10):\n",
    "        ep_ret, success = agent.test_model(env=env, steps=200, render_save_path=None)\n",
    "        eval_returns.append(ep_ret)\n",
    "        eval_successes.append(1 if success else 0)\n",
    "\n",
    "    eval_mean_r = float(np.mean(eval_returns))\n",
    "    eval_succ_rate = float(np.mean(eval_successes) * 100)\n",
    "    per_run_mean_rewards.append(eval_mean_r)\n",
    "    per_run_success_rates.append(eval_succ_rate)\n",
    "    print(f\"Run {run_idx} 評価結果: 平均報酬 {eval_mean_r:.3f}, 成功率 {eval_succ_rate:.1f}%\")\n",
    "\n",
    "    # Persist learning history for this run\n",
    "    history_path = results_dir / f\"GAIL_History_Run_{run_idx}.json\"\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"score_history\": score_history,\n",
    "            \"avg_score_history\": avg_score_history,\n",
    "            \"success_history\": success_history,\n",
    "            \"avg_success_history\": avg_success_history,\n",
    "            \"eval_mean_reward\": eval_mean_r,\n",
    "            \"eval_success_rate\": eval_succ_rate\n",
    "        }, f)\n",
    "\n",
    "    # Update manifest to keep track of all runs\n",
    "    run_manifest.append({\n",
    "        \"run_idx\": run_idx,\n",
    "        \"history_path\": str(history_path),\n",
    "        \"plot_path\": f\"../Results/GAIL/GAIL_Performance_Run_{run_idx}.png\",\n",
    "        \"model_path\": gail_agent_kwargs['model_save_path'],\n",
    "        \"eval_mean_reward\": eval_mean_r,\n",
    "        \"eval_success_rate\": eval_succ_rate,\n",
    "    })\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        json.dump(run_manifest, f, indent=2)\n",
    "\n",
    "    # Display the most recent learning curve inline\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(score_history, label='Score')\n",
    "    ax.plot(avg_score_history, label='Average Score')\n",
    "    ax.set_title(f'GAIL Run {run_idx} Learning Curve')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    display(Markdown(f\"### Run {run_idx} Learning Results\"))\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Summary over all automated runs\n",
    "overall_mean_reward = float(np.mean(per_run_mean_rewards)) if per_run_mean_rewards else 0.0\n",
    "overall_success_rate = float(np.mean(per_run_success_rates)) if per_run_success_rates else 0.0\n",
    "print(\"\\n===== GAIL Auto-Training Summary =====\")\n",
    "print(f\"Runs executed: {len(per_run_mean_rewards)} / {max_runs}\")\n",
    "print(f\"Average reward across runs: {overall_mean_reward:.3f}\")\n",
    "print(f\"Average success rate across runs: {overall_success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce364a",
   "metadata": {},
   "source": [
    "# GAILポリシーの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79754abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存済みの各Runの学習曲線をまとめて表示\n",
    "from glob import glob\n",
    "\n",
    "results_dir = Path(\"../Results/GAIL\")\n",
    "manifest_path = results_dir / \"run_manifest.json\"\n",
    "\n",
    "if not manifest_path.exists():\n",
    "    print(\"まだ保存済みのRunがありません。学習セルを先に実行してください。\")\n",
    "else:\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        run_manifest = json.load(f)\n",
    "\n",
    "    if len(run_manifest) == 0:\n",
    "        print(\"マニフェストが空です。学習セルを先に実行してください。\")\n",
    "    else:\n",
    "        for entry in run_manifest:\n",
    "            run_idx = entry.get(\"run_idx\")\n",
    "            history_path = entry.get(\"history_path\")\n",
    "            if not history_path or not Path(history_path).exists():\n",
    "                print(f\"Run {run_idx}: 履歴ファイルが見つかりませんでした ({history_path})\")\n",
    "                continue\n",
    "\n",
    "            with open(history_path, \"r\") as f:\n",
    "                hist = json.load(f)\n",
    "            score_history = hist.get(\"score_history\", [])\n",
    "            avg_score_history = hist.get(\"avg_score_history\", [])\n",
    "            success_history = hist.get(\"success_history\", [])\n",
    "            avg_success_history = hist.get(\"avg_success_history\", [])\n",
    "\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "            # 報酬\n",
    "            axes[0].plot(score_history, label='Score')\n",
    "            axes[0].plot(avg_score_history, label='Average Score')\n",
    "            axes[0].set_title(f'GAIL Run {run_idx} Learning Curve')\n",
    "            axes[0].set_xlabel('Episode')\n",
    "            axes[0].set_ylabel('Score')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "            # 成功率（存在する場合のみプロット）\n",
    "            if success_history:\n",
    "                axes[1].plot(success_history, label='Success')\n",
    "                axes[1].plot(avg_success_history, label='Average Success')\n",
    "                axes[1].set_ylim(0, 1.05)\n",
    "            axes[1].set_title('Episode Success (info[is_success])')\n",
    "            axes[1].set_xlabel('Episode')\n",
    "            axes[1].set_ylabel('Success Flag')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            display(Markdown(f\"### Run {run_idx} Learning Results\"))\n",
    "            display(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2721",
   "metadata": {},
   "source": [
    "学習済みGAILポリシーを実行し、GIFを保存して得られた報酬を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fd11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X connection to :0 broken (explicit kill or server shutdown).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "gif_base = '../Results/GAIL/GAIL Policy'\n",
    "gif_path = f\"{gif_base}.gif\"\n",
    "\n",
    "gail_reward, success = agent.test_model(env=env, steps=200, render_save_path=gif_base, fps=5)\n",
    "success_str = \"成功\" if success else \"失敗\"\n",
    "print(f'GAILの報酬: {gail_reward:.3f} - タスク: {success_str}')\n",
    "\n",
    "if Path(gif_path).exists():\n",
    "    display(Markdown('### GAILポリシーの実行結果 (GIF)'))\n",
    "    display(Image(filename=gif_path))\n",
    "else:\n",
    "    print(f\"GIF が見つかりませんでした: {gif_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

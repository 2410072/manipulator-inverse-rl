{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b401175",
   "metadata": {},
   "source": [
    "# ロボットアーム - Panda Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e009b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Markdown, Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from gail_algo import GAILTrainer, build_expert_loader\n",
    "from collect_expert_trajectories import collect_expert_trajectories\n",
    "\n",
    "# 日本語フォント設定\n",
    "import matplotlib\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8bdd3",
   "metadata": {},
   "source": [
    "* ロボット: Franka Emika Panda マニピュレータの操作タスクをシミュレート。\n",
    "<br>\n",
    "\n",
    "* 観測空間:\n",
    "    - すべてのタスクにグリッパの位置と速度（6値）。\n",
    "    - 物体を扱うタスクでは位置・姿勢・線形/回転速度が追加（物体1つにつき12値）。\n",
    "    - グリッパ開閉距離（拘束されていなければ1値）。\n",
    "\n",
    "* 行動空間:\n",
    "    - グリッパの平行移動コマンド（x, y, z の3値）。\n",
    "    - グリッパ開閉コマンド（1値）。\n",
    "    \n",
    "* シミュレーション:\n",
    "    - エージェントの1ステップあたり20タイムステップ（各2ms）。\n",
    "    - インタラクション周波数は25Hz。\n",
    "    - ほとんどのタスクは約2秒（50ステップ）のエピソード長。\n",
    "    \n",
    "* 報酬関数:\n",
    "    - 既定の報酬はスパース: 目標到達（5cm 以内）なら0、それ以外は -1。\n",
    "    - スパース報酬は定義が簡単だが、進捗の手掛かりが少ない。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97ae06",
   "metadata": {},
   "source": [
    "# PandaReach-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82d3d3",
   "metadata": {},
   "source": [
    "* ターゲット位置は30cm × 30cm × 30cmの範囲でランダム生成され、グリッパがそこへ到達するタスク。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c34656",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"PandaReach-v3\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    renderer=\"OpenGL\",\n",
    "    render_target_position=[0, 0.15, 0.25],\n",
    "    render_distance=0.85,\n",
    "    render_yaw=135,\n",
    "    render_pitch=-20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n観測空間:', env.observation_space)\n",
    "print('\\n行動空間: ', env.action_space)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print('\\n初期状態: ', (obs, info))\n",
    "\n",
    "# 行動は環境のアクション空間と同じ形状にする（スカラーだとエラーになる）\n",
    "action_sample = np.zeros(env.action_space.shape, dtype=np.float32)\n",
    "print('\\n環境での1ステップ: ', env.step(action_sample))\n",
    "\n",
    "print('\\n\\nレンダリングした環境: ')\n",
    "env.reset()\n",
    "plt.axis('off')\n",
    "plt.imshow(env.render())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = env.observation_space['observation'].shape[0] + \\\n",
    "            env.observation_space['achieved_goal'].shape[0] + \\\n",
    "            env.observation_space['desired_goal'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63aabd3",
   "metadata": {},
   "source": [
    "# GAIL用のエキスパート軌跡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd06b1",
   "metadata": {},
   "source": [
    "1. `../TD3/Models/Expert/` の事前学習TD3エキスパートで軌跡（状態 + 行動）をロールアウト。\n",
    "2. 生成した軌跡を一度 `./expert_trajectories.pt` に保存し、以降の実行で再利用。\n",
    "3. このファイルからミニバッチを作り、学習中にGAIL識別器へ供給する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867463e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_path = \"./expert_trajectories.pt\"\n",
    "if not os.path.exists(expert_path):\n",
    "    collect_expert_trajectories(env_name=\"PandaReach-v3\", episodes=200, steps_per_episode=300,\n",
    "                                expert_model_path=\"../TD3/Models/Expert/\", save_path=expert_path,\n",
    "                                render=False)\n",
    "else:\n",
    "    print(f\"キャッシュされたエキスパート軌跡を {expert_path} から利用します\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdeecab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expert_loader = build_expert_loader(expert_path, batch_size=256, device=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce94f4",
   "metadata": {},
   "source": [
    "# GAIL目的関数（数式）\n",
    "- **報酬（ポリシー側）**: $r(s,a) = \\log D_\\phi(s,a)$。識別器が「エキスパートらしい」と判断するほど報酬が高い。\n",
    "- **識別器の損失**（最小化で表記）: \n",
    "$$\\mathcal{L}_D = -\\mathbb{E}_{(s,a)\\sim\\text{expert}}[\\log D_\\phi(s,a)] - \\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log (1 - D_\\phi(s,a))]$$\n",
    "- **ポリシー（生成側）の目的**: \n",
    "$$\\max_\\theta\\; \\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log D_\\phi(s,a)]$$\n",
    "- **TD3本体**: Actor/Critic はTD3そのまま（2つのCriticで $\\min(Q_1, Q_2)$、ターゲット平滑化、遅延Actor更新）。GAILでは上記の $\\log D$ を外部報酬として TD3 学習に渡す点のみが異なる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAILエージェント設定（繰り返し実行用に辞書で保持）\n",
    "gail_agent_kwargs = dict(\n",
    "    env=env,\n",
    "    input_dims=obs_shape,\n",
    "    agent_name='GAIL',\n",
    "    model_save_path='./Models/Apprentice/',\n",
    "    exploration_period=300,\n",
    "    disc_lr=3e-4,\n",
    "    disc_updates=2,\n",
    "    gail_reward_scale=1.0,\n",
    "    expert_loader=expert_loader,\n",
    ")\n",
    "\n",
    "print('GAILエージェント設定を初期化しました。次のセルで繰り返し学習できます。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a59992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y/n で追加学習を繰り返し、各Runの結果を即座に可視化して保存する\n",
    "run_idx = 0\n",
    "keep_training = 'y'\n",
    "\n",
    "results_dir = Path(\"../Results/GAIL\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "manifest_path = results_dir / \"run_manifest.json\"\n",
    "\n",
    "# 既存のマニフェストを読み込み\n",
    "if manifest_path.exists():\n",
    "    try:\n",
    "        with open(manifest_path, \"r\") as f:\n",
    "            run_manifest = json.load(f)\n",
    "    except Exception:\n",
    "        run_manifest = []\n",
    "else:\n",
    "    run_manifest = []\n",
    "\n",
    "while keep_training == 'y':\n",
    "    run_idx += 1\n",
    "    # モデル保存先を run ごとに分けて上書きを避ける\n",
    "    gail_agent_kwargs['model_save_path'] = f\"./Models/Apprentice_Run_{run_idx}/\"\n",
    "\n",
    "    agent = GAILTrainer(**gail_agent_kwargs)\n",
    "\n",
    "    score_history, avg_score_history = agent.gail_train(\n",
    "        n_episodes=500,\n",
    "        opt_steps=10,\n",
    "        print_every=50,\n",
    "        render_save_path=None,\n",
    "        plot_save_path=f\"../Results/GAIL/GAIL_Performance_Run_{run_idx}.png\",\n",
    "    )\n",
    "\n",
    "    agent.save_model()\n",
    "    print(f\"Run {run_idx} を保存しました -> {gail_agent_kwargs['model_save_path']}\")\n",
    "\n",
    "    # 学習履歴を保存\n",
    "    history_path = results_dir / f\"GAIL_History_Run_{run_idx}.json\"\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump({\"score_history\": score_history, \"avg_score_history\": avg_score_history}, f)\n",
    "\n",
    "    # マニフェストを更新して保存\n",
    "    run_manifest.append({\n",
    "        \"run_idx\": run_idx,\n",
    "        \"history_path\": str(history_path),\n",
    "        \"plot_path\": f\"../Results/GAIL/GAIL_Performance_Run_{run_idx}.png\",\n",
    "        \"model_path\": gail_agent_kwargs['model_save_path'],\n",
    "    })\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        json.dump(run_manifest, f, indent=2)\n",
    "\n",
    "    # 直近Runのグラフを即座に表示\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.plot(score_history, label='Score')\n",
    "    ax.plot(avg_score_history, label='Average Score')\n",
    "    ax.set_title(f'GAIL Run {run_idx} 学習曲線')\n",
    "    ax.set_xlabel('エピソード')\n",
    "    ax.set_ylabel('スコア')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    display(Markdown(f\"### Run {run_idx} の学習結果\"))\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    keep_training = input('別のGAILエージェントを続けて学習しますか? (y/n): ').strip().lower()\n",
    "    if keep_training != 'y':\n",
    "        keep_training = 'n'\n",
    "        print('GAILの学習ループを終了します。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce364a",
   "metadata": {},
   "source": [
    "# GAILポリシーの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79754abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存済みの各Runの学習曲線をまとめて表示\n",
    "from glob import glob\n",
    "\n",
    "results_dir = Path(\"../Results/GAIL\")\n",
    "manifest_path = results_dir / \"run_manifest.json\"\n",
    "\n",
    "if not manifest_path.exists():\n",
    "    print(\"まだ保存済みのRunがありません。学習セルを先に実行してください。\")\n",
    "else:\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        run_manifest = json.load(f)\n",
    "\n",
    "    if len(run_manifest) == 0:\n",
    "        print(\"マニフェストが空です。学習セルを先に実行してください。\")\n",
    "    else:\n",
    "        for entry in run_manifest:\n",
    "            run_idx = entry.get(\"run_idx\")\n",
    "            history_path = entry.get(\"history_path\")\n",
    "            if not history_path or not Path(history_path).exists():\n",
    "                print(f\"Run {run_idx}: 履歴ファイルが見つかりませんでした ({history_path})\")\n",
    "                continue\n",
    "\n",
    "            with open(history_path, \"r\") as f:\n",
    "                hist = json.load(f)\n",
    "            score_history = hist.get(\"score_history\", [])\n",
    "            avg_score_history = hist.get(\"avg_score_history\", [])\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            ax.plot(score_history, label='Score')\n",
    "            ax.plot(avg_score_history, label='Average Score')\n",
    "            ax.set_title(f'GAIL Run {run_idx} 学習曲線 (保存済み)')\n",
    "            ax.set_xlabel('エピソード')\n",
    "            ax.set_ylabel('スコア')\n",
    "            ax.legend()\n",
    "            ax.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            display(Markdown(f\"### Run {run_idx} の学習結果 (保存済み)\"))\n",
    "            display(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2721",
   "metadata": {},
   "source": [
    "学習済みGAILポリシーを実行し、GIFを保存して得られた報酬を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_base = '../Results/GAIL/GAIL Policy'\n",
    "gif_path = f\"{gif_base}.gif\"\n",
    "\n",
    "gail_reward = agent.test_model(env=env, steps=200, render_save_path=gif_base, fps=5)\n",
    "print('GAILの報酬: ', gail_reward)\n",
    "\n",
    "if Path(gif_path).exists():\n",
    "    display(Markdown('### GAILポリシーの実行結果 (GIF)'))\n",
    "    display(Image(filename=gif_path))\n",
    "else:\n",
    "    print(f\"GIF が見つかりませんでした: {gif_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
